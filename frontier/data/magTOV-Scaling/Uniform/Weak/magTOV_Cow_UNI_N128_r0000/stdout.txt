$ mkdir -p magTOV_Cow_UNI_N128_r0000
$ cp magTOV_Cow_UNI_N128.par magTOV_Cow_UNI_N128_r0000
$ cd magTOV_Cow_UNI_N128_r0000
======================================================================
Launch MPI code...
----------------------------------------------------------------------
MPIRUN = srun /ccs/home/liwei/EinsteinToolkit/Cactus/exe/cactus_etk magTOV_Cow_UNI_N128.par

Job started on frontier00125 at Tue Apr  8 11:36:28 PM EDT 2025
======================================================================
--------------------------------------------------------------------------------

       10                                  
  1   0101       ************************  
  01  1010 10      The Cactus Code V4.16.0    
 1010 1101 011      www.cactuscode.org     
  1001 100101    ************************  
    00010101                               
     100011     (c) Copyright The Authors  
      0100      GNU Licensed. No Warranty  
      0101                                 
--------------------------------------------------------------------------------

Cactus version:    4.16.0
Compile date:      Apr 08 2025 (21:38:11)
Run date:          Apr 08 2025 (23:36:35-0400)
Run host:          frontier00125.frontier.olcf.ornl.gov (pid=1120895)
Working directory: /lustre/orion/ast182/scratch/liwei/simulations/magTOV-Scaling/Uniform/Weak/magTOV_Cow_UNI_N128_r0000
Executable:        /ccs/home/liwei/EinsteinToolkit/Cactus/exe/cactus_etk
Parameter file:    magTOV_Cow_UNI_N128.par
--------------------------------------------------------------------------------

Activating thorn Cactus...Success -> active implementation Cactus
Activation requested for 
--->ADMBaseX CarpetX HydroBaseX ODESolvers TmunuBaseX AsterX AsterSeeds TOVSolverX<---
Thorn ADMBaseX requests automatic activation of Loop
Thorn AsterSeeds requests automatic activation of EOSX
Thorn AsterSeeds requests automatic activation of AsterUtils
Thorn AsterX requests automatic activation of Con2PrimFactory
Thorn AsterX requests automatic activation of ReconX
Thorn CarpetX requests automatic activation of AMReX
Thorn CarpetX requests automatic activation of IOUtil
Thorn CarpetX requests automatic activation of MPI
Thorn CarpetX requests automatic activation of yaml_cpp
Thorn CarpetX requests automatic activation of zlib
Thorn CarpetX requests automatic activation of Arith
Thorn CarpetX requests automatic activation of CarpetXRegrid
Thorn CarpetX requests automatic activation of ADIOS2
Thorn CarpetX requests automatic activation of Silo
Thorn ODESolvers requests automatic activation of Subcycling
Thorn Con2PrimFactory requests automatic activation of Algo
Thorn MPI requests automatic activation of hwloc
Thorn Silo requests automatic activation of HDF5
Thorn Algo requests automatic activation of Boost
Activating thorn ADIOS2...Success -> active implementation ADIOS2
Activating thorn ADMBaseX...Success -> active implementation ADMBaseX
Activating thorn Algo...Success -> active implementation Algo
Activating thorn AMReX...Success -> active implementation AMReX
Activating thorn Arith...Success -> active implementation Arith
Activating thorn AsterSeeds...Success -> active implementation AsterSeeds
Activating thorn AsterUtils...Success -> active implementation AsterUtils
Activating thorn AsterX...Success -> active implementation AsterX
Activating thorn Boost...Success -> active implementation Boost
Activating thorn CarpetX...Success -> active implementation Driver
Activating thorn CarpetXRegrid...Success -> active implementation CarpetXRegrid
Activating thorn Con2PrimFactory...Success -> active implementation Con2PrimFactory
Activating thorn EOSX...Success -> active implementation EOSX
Activating thorn HDF5...Success -> active implementation HDF5
Activating thorn hwloc...Success -> active implementation hwloc
Activating thorn HydroBaseX...Success -> active implementation HydroBaseX
Activating thorn IOUtil...Success -> active implementation IO
Activating thorn Loop...Success -> active implementation Loop
Activating thorn MPI...Success -> active implementation MPI
Activating thorn ODESolvers...Success -> active implementation ODESolvers
Activating thorn ReconX...Success -> active implementation ReconX
Activating thorn Silo...Success -> active implementation Silo
Activating thorn Subcycling...Success -> active implementation Subcycling
Activating thorn TmunuBaseX...Success -> active implementation TmunuBaseX
Activating thorn TOVSolverX...Success -> active implementation TOVSolverX
Activating thorn yaml_cpp...Success -> active implementation yaml_cpp
Activating thorn zlib...Success -> active implementation zlib
--------------------------------------------------------------------------------
  if (recover initial data)
    Recover parameters
  endif

  Startup routines
    [CCTK_STARTUP]
      CarpetX::Driver_Startup: Start up the driver
      IOUtil::IOUtil_Startup: Startup routine
      GROUP hwloc_startup: hwloc startup group
        hwloc::hwloc_version: Output hwloc version

  Startup routines which need an existing grid hierarchy
    [CCTK_WRAGH]
      Algo::Test_roots: [meta] Test root-finding algorithms
      Arith::Test_smallvector: [meta] Test small vectors
      Arith::Test_spvect: [meta] Test sparse vectors
      AsterX::AsterX_EstimateError_Setup: [global] set up for calculating regrid error
      Con2PrimFactory::Con2PrimFactory_Test: [meta] Con2Prim Self-test
      TOVSolverX::TOVX_C_AllocateMemory: [global] Allocate memory for 1D TOV variables
      TOVSolverX::TOVX_C_Integrate_RHS: [global] Integrate the 1D variables for the TOV star
  Parameter checking routines
    [CCTK_PARAMCHECK]

  Initialisation
    if (NOT (recover initial data AND recovery_mode is 'strict'))
      [CCTK_PREREGRIDINITIAL]
      Set up grid hierarchy
      [CCTK_POSTREGRIDINITIAL]
      [CCTK_BASEGRID]
        CarpetXRegrid::CarpetXRegrid_InitError: Initialize regridding error to zero
        GROUP ODESolvers_BaseGrid: Set up constants (e.g. coordinates) everywhere on the grid
      [CCTK_INITIAL]
        GROUP ODESolvers_Initial: Set up initial conditions on interior of state vector
          GROUP ADMBaseX_InitialData: Schedule group for calculating ADM initial data
          GROUP ADMBaseX_InitialGauge: Schedule group for the ADM initial gauge condition
            ADMBaseX::ADMBaseX_initial_dtlapse: Set dtlapse to zero
            ADMBaseX::ADMBaseX_initial_dtshift: Set dtshift to zero
          GROUP ADMBaseX_PostInitial: Schedule group for modifying the ADM initial data, such as e.g. adding noise
          GROUP HydroBaseX_InitialData: Schedule group for calculating hydro initial data
            HydroBaseX::HydroBaseX_initial_data: Set up vacuum initial data
            TOVSolverX::TOVX_C_Exact: Set values for all variables of TOV
            TOVSolverX::TOVX_C_Exact_Interpolation_C2V: Interpolate metric from cell center to vertex
          GROUP HydroBaseX_PostInitial: Schedule group for modifying the hydro initial data, such as e.g. adding noise
            AsterSeeds::SetEntropy: Set initial entropy
          AsterSeeds::AsterSeeds_InitializeCenteredAvec_TOV: Set up initial conditions for the cell-centered vector potential
          AsterSeeds::AsterSeeds_InitializeStagAvec_TOV: Set up initial conditions for the vector potential
          GROUP AsterX_InitialGroup: Initialize conservative variables
            AsterX::AsterX_ComputedBstagFromA: Calculate dBstag from curl of A
            AsterX::AsterX_ComputedBFromdBstag: Calculate centered dB from dBstag
            AsterX::AsterX_ComputeBFromdB: Calculate centered B from densitized B
            AsterX::AsterX_CheckPrims: Enforce limits on primitive variables
            AsterX::AsterX_Prim2Con_Initial: Compute conserved variables from primitive variables at initial
            AsterX::AsterX_PsiZero_Initial: Set Psi, time component of 4-vector potential, to zero initially
        GROUP TmunuBaseX_SetTmunuVars: Schedule group for setting T_munu
          TmunuBaseX::TmunuBaseX_ZeroTmunu: Set T_munu to zero
          GROUP TmunuBaseX_AddToTmunu: Add to T_munu here
      [CCTK_POSTINITIAL]
        GROUP ODESolvers_PostStep: Apply boundary conditions to state vector, and project if necessary
          GROUP ADMBaseX_SetADMVars: Set ADM variables in this group
          GROUP ADMBaseX_SetADMRHS: Set ADM RHS variables in this group
          AsterX::AsterX_Sync: [global] Synchronize
          GROUP AsterX_Con2PrimGroup: Compute primitive variables
            AsterX::AsterX_ComputedBstagFromA: Calculate dBstag from curl of A
            AsterX::AsterX_ComputedBFromdBstag: Calculate centered dB from dBstag
            AsterX::AsterX_Con2Prim: Calculate primitive variables from conservative variables
          GROUP HydroBaseX_SetHydroVars: Set hydro variables in this group, or before this group
          GROUP TmunuBaseX_SetTmunuVars: Schedule group for setting T_munu
            TmunuBaseX::TmunuBaseX_ZeroTmunu: Set T_munu to zero
            GROUP TmunuBaseX_AddToTmunu: Add to T_munu here
        GROUP ODESolvers_EstimateError: Estimate discretization error (for regridding)
          AsterX::AsterX_EstimateError: Estimate local error for regridding initial conditions
      Initialise finer grids recursively
      Restrict from finer grids
      [CCTK_POSTRESTRICTINITIAL]
      [CCTK_POSTPOSTINITIAL]
      [CCTK_POSTSTEP]
        GROUP ODESolvers_EstimateError: Estimate discretization error (for regridding)
          AsterX::AsterX_EstimateError: Estimate local error for regridding initial conditions
    endif
    if (recover initial data)
      [CCTK_BASEGRID]
        CarpetXRegrid::CarpetXRegrid_InitError: Initialize regridding error to zero
        GROUP ODESolvers_BaseGrid: Set up constants (e.g. coordinates) everywhere on the grid
      [CCTK_RECOVER_VARIABLES]
      [CCTK_POST_RECOVER_VARIABLES]
        GROUP ODESolvers_PostStep: Apply boundary conditions to state vector, and project if necessary
          GROUP ADMBaseX_SetADMVars: Set ADM variables in this group
          GROUP ADMBaseX_SetADMRHS: Set ADM RHS variables in this group
          AsterX::AsterX_Sync: [global] Synchronize
          GROUP AsterX_Con2PrimGroup: Compute primitive variables
            AsterX::AsterX_ComputedBstagFromA: Calculate dBstag from curl of A
            AsterX::AsterX_ComputedBFromdBstag: Calculate centered dB from dBstag
            AsterX::AsterX_Con2Prim: Calculate primitive variables from conservative variables
          GROUP HydroBaseX_SetHydroVars: Set hydro variables in this group, or before this group
          GROUP TmunuBaseX_SetTmunuVars: Schedule group for setting T_munu
            TmunuBaseX::TmunuBaseX_ZeroTmunu: Set T_munu to zero
            GROUP TmunuBaseX_AddToTmunu: Add to T_munu here
        GROUP ODESolvers_EstimateError: Estimate discretization error (for regridding)
          AsterX::AsterX_EstimateError: Estimate local error for regridding initial conditions
    endif
    if (checkpoint initial data)
      [CCTK_CPINITIAL]
        CarpetX::CarpetX_CheckpointInitial: [meta] Checkpoint initial conditions
    endif
    if (analysis)
      [CCTK_ANALYSIS]
        GROUP ODESolvers_Analysis: Calculate analysis quantities everywhere
  endif
  Output grid variables

  do loop over timesteps
    [CCTK_PREREGRID]
    Change grid hierarchy
    [CCTK_POSTREGRID]
      AsterX::AsterX_Sync: [global] Synchronize
      TOVSolverX::TOVX_C_Exact_ADM: Set values for ADM variables of TOV
      TOVSolverX::TOVX_C_Exact_ADM_Interpolation_C2V: Interpolate metric from cell center to vertex
      GROUP ODESolvers_PostStep: Apply boundary conditions to state vector, and project if necessary
        GROUP ADMBaseX_SetADMVars: Set ADM variables in this group
        GROUP ADMBaseX_SetADMRHS: Set ADM RHS variables in this group
        AsterX::AsterX_Sync: [global] Synchronize
        GROUP AsterX_Con2PrimGroup: Compute primitive variables
          AsterX::AsterX_ComputedBstagFromA: Calculate dBstag from curl of A
          AsterX::AsterX_ComputedBFromdBstag: Calculate centered dB from dBstag
          AsterX::AsterX_Con2Prim: Calculate primitive variables from conservative variables
        GROUP HydroBaseX_SetHydroVars: Set hydro variables in this group, or before this group
        GROUP TmunuBaseX_SetTmunuVars: Schedule group for setting T_munu
          TmunuBaseX::TmunuBaseX_ZeroTmunu: Set T_munu to zero
          GROUP TmunuBaseX_AddToTmunu: Add to T_munu here
    Rotate timelevels
    iteration = iteration+1
    t = t+dt
    [CCTK_PRESTEP]
    [CCTK_EVOL]
      ODESolvers::ODESolvers_Solve: [level] Solve ODEs
    Evolve finer grids recursively
    Restrict from finer grids
    [CCTK_POSTRESTRICT]
      GROUP ODESolvers_PostStep: Apply boundary conditions to state vector, and project if necessary
        GROUP ADMBaseX_SetADMVars: Set ADM variables in this group
        GROUP ADMBaseX_SetADMRHS: Set ADM RHS variables in this group
        AsterX::AsterX_Sync: [global] Synchronize
        GROUP AsterX_Con2PrimGroup: Compute primitive variables
          AsterX::AsterX_ComputedBstagFromA: Calculate dBstag from curl of A
          AsterX::AsterX_ComputedBFromdBstag: Calculate centered dB from dBstag
          AsterX::AsterX_Con2Prim: Calculate primitive variables from conservative variables
        GROUP HydroBaseX_SetHydroVars: Set hydro variables in this group, or before this group
        GROUP TmunuBaseX_SetTmunuVars: Schedule group for setting T_munu
          TmunuBaseX::TmunuBaseX_ZeroTmunu: Set T_munu to zero
          GROUP TmunuBaseX_AddToTmunu: Add to T_munu here
    [CCTK_POSTSTEP]
      GROUP ODESolvers_EstimateError: Estimate discretization error (for regridding)
        AsterX::AsterX_EstimateError: Estimate local error for regridding initial conditions
    if (checkpoint)
      [CCTK_CHECKPOINT]
      CarpetX::CarpetX_Checkpoint: [meta] Checkpoint
    endif
    if (analysis)
      [CCTK_ANALYSIS]
      GROUP ODESolvers_Analysis: Calculate analysis quantities everywhere
    endif
    Output grid variables
    enddo

  Termination routines
    [CCTK_TERMINATE]
      CarpetX::CarpetX_CheckpointTerminate: [meta] Checkpoint before terminating
      TOVSolverX::TOVX_C_FreeMemory: [global] Free memory from TOVSolverX_C at the end of the simulation

  Shutdown routines
    [CCTK_SHUTDOWN]
      CarpetX::Driver_Shutdown: [meta] Shut down the driver

  Routines run after changing the grid hierarchy:
    [CCTK_POSTREGRID]
      AsterX::AsterX_Sync: [global] Synchronize
      TOVSolverX::TOVX_C_Exact_ADM: Set values for ADM variables of TOV
      TOVSolverX::TOVX_C_Exact_ADM_Interpolation_C2V: Interpolate metric from cell center to vertex
      GROUP ODESolvers_PostStep: Apply boundary conditions to state vector, and project if necessary
        GROUP ADMBaseX_SetADMVars: Set ADM variables in this group
        GROUP ADMBaseX_SetADMRHS: Set ADM RHS variables in this group
        AsterX::AsterX_Sync: [global] Synchronize
        GROUP AsterX_Con2PrimGroup: Compute primitive variables
          AsterX::AsterX_ComputedBstagFromA: Calculate dBstag from curl of A
          AsterX::AsterX_ComputedBFromdBstag: Calculate centered dB from dBstag
          AsterX::AsterX_Con2Prim: Calculate primitive variables from conservative variables
        GROUP HydroBaseX_SetHydroVars: Set hydro variables in this group, or before this group
        GROUP TmunuBaseX_SetTmunuVars: Schedule group for setting T_munu
          TmunuBaseX::TmunuBaseX_ZeroTmunu: Set T_munu to zero
          GROUP TmunuBaseX_AddToTmunu: Add to T_munu here
--------------------------------------------------------------------------------
INFO (hwloc): library version 2.11.1, API version 0x20b00
--------------------------------------------------------------------------------
                                                            
   _______ _______  ______  _____  _______ _______ _     _  
   |       |_____| |_____/ |_____] |______    |     \___/   
   |_____  |     | |    \_ |       |______    |    _/   \_  
                                                            
AMR driver provided by CarpetX,
using AMReX 25.04-10-gb20bef732a18 (MPI, no OpenMP, accelerators, optimized)
--------------------------------------------------------------------------------

Initializing AMReX (25.04-10-gb20bef732a18)...
MPI initialized with 1024 MPI processes
MPI initialized with thread support level 2
Initializing HIP...
HIP initialized with 1024 devices.
AMReX (25.04-10-gb20bef732a18) initialized
Error: MPI job failed.
